# -*- coding: utf-8 -*-
"""Stability.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13x7CflzKlGqGhFwk_uolncMZH4YTfPew

### Import Necessary Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
#Creation of Numpy Arrays,Scientific computing and linear Algebra.  
import numpy as np     

#For Loading DataSet,Data Manipulation,Data cleaning and Stastics Analysis.
import pandas as pd

#For Visualization
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

"""### Loading Dataset"""

from google.colab import drive
drive._mount('/content/drive')

data=pd.read_csv('/content/stability_database.csv')

data.head()

"""### Shape of the Dataset."""

data.head()

"""### Displaying the Column Names"""

data.columns

"""### Finding Stastical Summary"""

data.describe().T

"""### Finding Duplicates rows."""

data.duplicated().any()

"""There is no duplicate rows in the dataset

### Finding Column Names,Non-Null values and Datatypes.
"""

data.info()

"""### Finding Nullvalues."""

data.isnull().any()

"""### Finding data types"""

data.dtypes

"""### Describing Dataset with object Datatype."""

data.select_dtypes(include=['object'])

"""### Converting Object Datatype to Numerical Datatype."""

# convert A column is object type to category type
data["A"] = data["A"].astype('category')
data["A"] = data["A"].cat.codes

# convert A' column is object type to category type
data["A'"] = data["A'"].astype('category')
data["A'"] = data["A'"].cat.codes

# convert B column is object type to category type
data["B"] = data["B"].astype('category')
data["B"] = data["B"].cat.codes

# convert B' column is object type to category type
data["Be"] = data["Be"].astype('category')
data["Be"] = data["Be"].cat.codes

# convert X1 column is object type to category type
data["X1"] = data["X1"].astype('category')
data["X1"] = data["X1"].cat.codes

# convert type column is object type to category type
data["type"] = data["type"].astype('category')
data["type"] = data["type"].cat.codes

# convert functional group column is object type to category type
data["functional group"] = data["functional group"].astype('category')
data["functional group"] = data["functional group"].cat.codes

"""### After Converting Checking the datatypes."""

data.dtypes

data.head()

data.rename(columns={"Œºùêµ ÃÖ	":"m"},inplace=True)

data['AOS'] = data['A_OS']+data["A'_OS"]
data['AA'] = data['A']+data["A'"]
data['BB'] = data['B']+data["Be"]
data['A_HOMO'] = data['A_HOMO-']+data["A_HOMO+"]
data['A_IE'] = data['A_IE-']+data["A_IE+"]
data['A_LUMO'] = data['A_LUMO-']+data["A_LUMO+"]
data['A_X'] = data['A_X-']+data["A_X+"]
data['A_Z_radii'] = data['A_Z_radii-']+data["A_Z_radii+"]
data['A_e_affin'] = data['A_e_affin-']+data["A_e_affin+"]
data['BOS'] = data['B_OS']+data["B'_OS"]
data['B_HOMO'] = data['B_HOMO-']+data["B_HOMO+"]
data['B_IE'] = data['B_IE-']+data["B_IE+"]
data['B_LUMO'] = data['B_LUMO-']+data["B_LUMO+"]
data['B_X'] = data['B_X-']+data["B_X+"]
data['B_Z_radii'] = data['B_Z_radii-']+data["B_Z_radii+"]
data['B_e_affin'] = data['B_e_affin-']+data["B_e_affin+"]



data.columns

data = data[['functional group','AOS', 'AA', 'A_HOMO', 'A_IE', 'A_LUMO', 'A_X', 'A_Z_radii',
       'A_e_affin', 'BOS', 'B_HOMO', 'B_IE', 'B_LUMO', 'B_X', 'B_Z_radii',
       'B_e_affin','X1','X1_OS', 'e_above_hull', 'Œº', 'ŒºƒÄ', "muB",'stable', 't', 'type']]

data

data.shape

"""### Spittng the dataset and choosing the Target Column"""

X = data.drop(["stable"],axis=1) 
y = data['stable']

"""### Heatmap For Correlation"""

plt.figure(figsize=(15,10))
sns.heatmap(data= data.corr(), annot=True, cmap='viridis')

"""### Correlation with independent variable"""

X.corrwith(y).plot.bar(figsize = (15, 10), title = "Correlation with Target", fontsize = 10,grid = True)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
plt.rcParams.update({'figure.figsize': (12.0, 8.0)})
plt.rcParams.update({'font.size': 14})

"""### Splitting Data"""

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=12)

"""### ExtraTrees Classifier"""

et = ExtraTreesClassifier(n_estimators=100)
et.fit(X_train, y_train)

et.feature_importances_

"""### Calculating the imfortance of each feature using ET"""

plt.barh(X.columns, et.feature_importances_)

sorted_idx = et.feature_importances_.argsort()
plt.barh(X.columns[sorted_idx], et.feature_importances_[sorted_idx])
plt.title("Feature Importance")
plt.xlabel("Importance")

X.columns[sorted_idx]

import seaborn as sns
#get correlations of each features in dataset
corrmat = data.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap='viridis')

# Importing standardscalar module
from sklearn.preprocessing import StandardScaler

scalar = StandardScaler()

# fitting
scalar.fit(X)
X = scalar.transform(X)
X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
print('X_train size: {}, X_test size: {}'.format(X_train.shape, X_test.shape))

import sklearn.metrics as metrics
from sklearn.model_selection import cross_val_score

r=[]
f=[]
k=[]
l=[]
index=0
for key in X_train.columns:
    index=index+1
    l.append(index)
    r.append(key)
    k.append(key)
    X_train1=X_train[r]
    X_test1=X_test[r]
    print('Selected Feature: ', index)
    print('Column Names:',r)
    clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)
    clf.fit(X_train1, y_train)
    y_pred = clf.predict(X_test1)
    acc=accuracy_score(y_test, y_pred)
    print('Accuracy:',acc)
    f.append(acc)

randf = pd.DataFrame(list(zip(l, f)),columns =['features', 'r_accuracy'])
randf

randf.plot(kind = 'line',
        x = 'features',
        y = 'r_accuracy',
        color = 'green')

from sklearn.ensemble import GradientBoostingClassifier
r=[]
f=[]
k=[]
l=[]
index=0
for key in X_train.columns:
    index=index+1
    r.append(key)
    l.append(index)
    k.append(key)
    X_train1=X_train[r]
    X_test1=X_test[r]
    print('Selected Feature: ', index)
    print('Column Names:',r)
    clf = GradientBoostingClassifier(n_estimators=100, random_state=0)
    clf.fit(X_train1, y_train)
    y_pred = clf.predict(X_test1)
    acc=accuracy_score(y_test, y_pred)
    print('Accuracy:',acc)
    f.append(acc)

gb = pd.DataFrame(list(zip(l, f)),columns =['features', 'g_accuracy'])
gb

gb.plot(kind = 'line',
        x = 'features',
        y = 'g_accuracy',
        color = 'blue')

r=[]
f=[]
k=[]
l=[]
index=0
for key in X_train.columns:
    index=index+1
    r.append(key)
    l.append(index)
    k.append(key)
    X_train1=X_train[r]
    X_test1=X_test[r]
    print('Selected Feature: ', index)
    print('Column Names:',r)
    clf = ExtraTreesClassifier(n_estimators=100, random_state=0, n_jobs=-1)
    clf.fit(X_train1, y_train)
    y_pred = clf.predict(X_test1)
    acc=accuracy_score(y_test, y_pred)
    print('Accuracy:',acc)
    f.append(acc)

ext = pd.DataFrame(list(zip(l, f)),columns =['features', 'e_accuracy'])
ext

ext.plot(kind = 'line',
        x = 'features',
        y = 'e_accuracy',
        color = 'orange')

from sklearn.svm import LinearSVC
r=[]
f=[]
k=[]
l=[]
index=0
for key in X_train.columns:
    index=index+1
    r.append(key)
    l.append(index)
    k.append(key)
    X_train1=X_train[r]
    X_test1=X_test[r]
    print('Selected Feature: ', index)
    print('Column Names:',r)
    clf = LinearSVC(random_state=0)
    clf.fit(X_train1, y_train)
    y_pred = clf.predict(X_test1)
    acc=accuracy_score(y_test, y_pred)
    print('Accuracy:',acc)
    print()
    f.append(acc)

svc = pd.DataFrame(list(zip(l, f)),columns =['features', 'svc_accuracy'])
svc

svc.plot(kind = 'line',
        x = 'features',
        y = 'svc_accuracy',
        color = 'black')

from sklearn.naive_bayes import GaussianNB
r=[]
f=[]
k=[]
l=[]
index=0
for key in X_train.columns:
    index=index+1
    r.append(key)
    l.append(index)
    k.append(key)
    X_train1=X_train[r]
    X_test1=X_test[r]
    print('Selected Feature: ', index)
    print('Column Names:',r)
    clf = GaussianNB()
    clf.fit(X_train1, y_train)
    y_pred = clf.predict(X_test1)
    acc=accuracy_score(y_test, y_pred)
    print('Accuracy:',acc)
    f.append(acc)

nb = pd.DataFrame(list(zip(l, f)),columns =['features', 'nb_accuracy'])
nb

nb.plot(kind = 'line',
        x = 'features',
        y = 'nb_accuracy',
        color = 'yellow')

all_alg=randf

all_alg['e_accuracy']=ext['e_accuracy']
all_alg['svc_accuracy']=svc['svc_accuracy']
all_alg['nb_accuracy']=nb['nb_accuracy']
all_alg['g_accuracy']=gb['g_accuracy']

all_alg.plot(kind = 'line',
        x = 'features',
        y = ['nb_accuracy','e_accuracy','svc_accuracy','g_accuracy'])

# Importing standardscalar module
from sklearn.preprocessing import StandardScaler

scalar = StandardScaler()

# fitting
scalar.fit(X)
scaled_data = scalar.transform(X)

# Importing PCA
from sklearn.decomposition import PCA

# Let's say, components = 2
pca = PCA(n_components = 2)
pca.fit(scaled_data)
x_pca = pca.transform(scaled_data)

x_pca.shape

# giving a larger plot
plt.figure(figsize =(8, 6))

plt.scatter(x_pca[:, 0], x_pca[:, 1], c = y, cmap ='plasma')

# labeling x and y axes
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')

from sklearn.decomposition import KernelPCA
kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)
x_kpca = kpca.fit_transform(X)
principalComponents = kpca.fit_transform(X)

final_df = pd.DataFrame(data = principalComponents
                 , columns = ['PC-1', 'PC-2'])
final_df['stable']=y

final_df.head()

X = final_df.iloc[:,0:2]  #independent columns
y = final_df.iloc[:,-1]    #target column

X.head()

y.head()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
print('X_train size: {}, X_test size: {}'.format(X_train.shape, X_test.shape))

"""### RandomForestClassifier"""

clf=RandomForestClassifier()
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
rfc_cv_score = cross_val_score(clf, X, y, cv=5)
rfc_cv_score

"""### ExtraTreesClassifier"""

clf = ExtraTreesClassifier()
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
et=metrics.accuracy_score(y_test, y_pred)
et
rfc_cv_score = cross_val_score(clf, X, y, cv=5)
rfc_cv_score

et=metrics.accuracy_score(y_test, y_pred)
et

"""### GradientBoostingClassifier"""

clf=GradientBoostingClassifier()
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
gb=metrics.accuracy_score(y_test, y_pred)
gb
rfc_cv_score = cross_val_score(clf, X, y, cv=5)
rfc_cv_score

gb=metrics.accuracy_score(y_test, y_pred)
gb

"""### Support vector classifier."""

svc=SVC() #Default hyperparameters
svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
sv=metrics.accuracy_score(y_test, y_pred)
sv
rfc_cv_score = cross_val_score(svc, X, y, cv=5)
rfc_cv_score

sv=metrics.accuracy_score(y_test, y_pred)
sv

"""### Naive bayes."""

gnb = GaussianNB()
gnb.fit(X_train,y_train)
y_pred=svc.predict(X_test)
nb=metrics.accuracy_score(y_test, y_pred)
nb
rfc_cv_score = cross_val_score(gnb, X, y, cv=5)
rfc_cv_score

nb=metrics.accuracy_score(y_test, y_pred)
nb

"""### Applying Shallow Neural Network."""

data

X=data.drop(["stable"],axis=1)
Y=data['stable']

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20,random_state = 25)

print("shape of X_train:{} shape 0f Y_train:{}".format(X_train.shape, Y_train.shape))  
print("shape of X_test:{} shape 0f Y_test:{}".format(X_test.shape, Y_test.shape))

X_train = X_train.T

Y_train = Y_train.values.reshape(1, len(Y_train))

X_test = X_test.T

Y_test = Y_test.values.reshape(1, len(Y_test))

print("shape of X_train:{} shape 0f Y_train:{} after transformation".format(X_train.shape, Y_train.shape))

import tensorflow.compat.v1 as tf
tf.compat.v1.disable_eager_execution()

#import tensorflow as tf

def placeholders(num_features):
    A_0 = tf.compat.v1.placeholder(dtype = tf.float64, shape = ([num_features,None]))
    Y = tf.compat.v1.placeholder(dtype = tf.float64, shape = ([1,None]))
    return A_0,Y

def initialiseParameters(num_features, num_nodes):
    W1 = tf.Variable(initial_value=tf.random_normal([num_nodes,num_features], dtype = tf.float64) * 0.01)
    b1 = tf.Variable(initial_value=tf.zeros([num_nodes,1], dtype=tf.float64))
    W2 = tf.Variable(initial_value=tf.random_normal([1,num_nodes], dtype=tf.float64) * 0.01)
    b2 = tf.Variable(initial_value=tf.zeros([1,1], dtype=tf.float64))
    return {"W1":W1,"b1":b1,"W2":W2,"b2":b2}

def forward_propagation(A_0,parameters):
    Z1 = tf.matmul(parameters["W1"],A_0) + parameters["b1"]
    A1 = tf.nn.relu(Z1)
    Z2 = tf.matmul(parameters["W2"],A1) + parameters["b2"]
    return Z2

def shallow_model(X_train,Y_train,X_test,Y_test, num_nodes, learning_rate, num_iter):
    num_features = X_train.shape[0]
    A_0, Y = placeholders(num_features)
    parameters = initialiseParameters(num_features, num_nodes)
    Z2 = forward_propagation(A_0, parameters)
    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Z2,labels=Y))
    train_net = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
    init = tf.global_variables_initializer()
    with tf.Session() as sess:
        sess.run(init)
        for i in range(num_iter):
            _,c = sess.run([train_net, cost], feed_dict={A_0: X_train, Y: Y_train})
            if i % 1000 == 0:
                print(c)
        correct_prediction = tf.equal(tf.round(tf.sigmoid(Z2)), Y)
           # Calculate accuracy
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
        print("Accuracy on test set:", accuracy.eval({A_0: X_test, Y: Y_test}))

shallow_model(X_train, Y_train, X_test, Y_test, 16, 0.2, 10000)



